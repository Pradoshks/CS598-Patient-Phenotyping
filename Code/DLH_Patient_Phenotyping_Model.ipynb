{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qinalan10/cs598-patient-phenotyping/blob/master/Code/DLH_Patient_Phenotyping_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading a Pre-Trained CNN Model for Patient Phenotyping Based on Patient Notes\n",
        "\n",
        "This  notebook is to load a pre-trained model and test data and run the evaluation. \n",
        "\n",
        "*   Read the file containing the test data only.\n",
        "\n",
        "*   Create CNN Classfier\n",
        "*   Load the model\n",
        "*   Test Model Perfromance using \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LMdndDXF8WaH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcVwbjZ1SlkC",
        "outputId": "6d8901bc-6ce3-4749-9bfc-9e697afc6b07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# importing libaries\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import pandas as pd \n",
        "from torchsummary import summary\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import re \n",
        "import numpy as np \n",
        "from sklearn.metrics import classification_report\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score   \n",
        "from torchsummary import summary\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
        "import pickle\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting Paths & Read the Final File"
      ],
      "metadata": {
        "id": "M6pll-np_auZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LFSFksFq9uh"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/drive/MyDrive/Data/dataset_dict.pickle\"\n",
        "# Model Path\n",
        "model_path = '/content/drive/MyDrive/Data/trained_model.pth'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    texts = [item['text'] for item in batch]  # Extract the text data from the batch\n",
        "    labels = [item['labels'] for item in batch]  # Extract the labels from the batch\n",
        "\n",
        "    # Find the maximum length of text sequences in the batch\n",
        "    max_len = max([len(text) for text in texts])\n",
        "\n",
        "    # Pad the text sequences with a padding token (e.g., '<pad>') to the maximum length\n",
        "    padded_texts = []\n",
        "    for text in texts:\n",
        "      padded_text = text + ['<pad>'] * (max_len - len(text))\n",
        "      padded_texts.append(padded_text)\n",
        "\n",
        "    # Convert the labels to a PyTorch tensor\n",
        "    labels = torch.stack(labels)\n",
        "\n",
        "    return {'text': padded_texts, 'labels': labels}"
      ],
      "metadata": {
        "id": "rykYrPCqaU1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the test data."
      ],
      "metadata": {
        "id": "CtwlqSSfbMo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(dataset_path, 'rb') as file:\n",
        "    dataset_dict = pickle.load(file)\n",
        "test_dataset = dataset_dict[\"test_dataset\"]\n",
        "batch_size = 64\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "cvVvKQOPXB9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the CNN Classifier - Based on which the model was trained."
      ],
      "metadata": {
        "id": "rhxwjxVh9wSZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1Sw8yAdvGls"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random \n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_filters, filter_sizes, embeddings, output_size, w2v_dictionary):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        self.dictionary = w2v_dictionary\n",
        "        self.embedding = nn.Embedding.from_pretrained(embeddings)#(vocab_size, embedding_dim)\n",
        "        self.conv1 = nn.Conv1d(embedding_dim, num_filters, filter_sizes[0])\n",
        "        self.conv2 = nn.Conv1d(embedding_dim, num_filters, filter_sizes[1])\n",
        "        self.conv3 = nn.Conv1d(embedding_dim, num_filters, filter_sizes[2])\n",
        "        self.conv4 = nn.Conv1d(embedding_dim, num_filters, filter_sizes[3])\n",
        "        self.dropout = nn.Dropout(p = 0.1)\n",
        "        self.fc = nn.Linear(len(filter_sizes) * num_filters, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "      # change to indexes, then to tensor\n",
        "        z2 = []\n",
        "        for ls in x:\n",
        "          z1 = []\n",
        "          for word in ls:\n",
        "            if word not in self.dictionary.keys():\n",
        "              random_key = random.sample(list(self.dictionary.keys()), 1)[0]\n",
        "              z1.append(self.dictionary[random_key])\n",
        "            else:\n",
        "              z1.append(self.dictionary[word])\n",
        "          z2.append(z1)  \n",
        "\n",
        "        x = torch.tensor(z2, device = device)\n",
        "        x = self.embedding(x)\n",
        "        x = x.permute(0, 2, 1)  # Permute the dimensions for Conv1d (batch_size, embedding_dim, sequence_length)\n",
        "\n",
        "        x1 = F.relu(self.conv1(x))\n",
        "        x1 = F.max_pool1d(x1 , x1.size(2)).squeeze(2)\n",
        "\n",
        "        x2 = F.relu(self.conv2(x))\n",
        "        x2 = F.max_pool1d(x2 , x2.size(2)).squeeze(2)\n",
        "\n",
        "        x3 = F.relu(self.conv3(x))\n",
        "        x3 = F.max_pool1d(x3 , x3.size(2)).squeeze(2)\n",
        "\n",
        "        x4 = F.relu(self.conv4(x))\n",
        "        x4 = F.max_pool1d(x4 , x4.size(2)).squeeze(2)\n",
        "\n",
        "        out = torch.cat((x1, x2, x3, x4), 1)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        out = torch.sigmoid(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Pre-Trained Model"
      ],
      "metadata": {
        "id": "CKrAZx6Kbao1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "trained_model = torch.load(model_path)\n",
        "print(trained_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TolPup9E1xFL",
        "outputId": "3c2ca9de-93a6-4dec-f3a3-cb68948b8af1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNClassifier(\n",
            "  (embedding): Embedding(10316, 300)\n",
            "  (conv1): Conv1d(300, 256, kernel_size=(1,), stride=(1,))\n",
            "  (conv2): Conv1d(300, 256, kernel_size=(2,), stride=(1,))\n",
            "  (conv3): Conv1d(300, 256, kernel_size=(3,), stride=(1,))\n",
            "  (conv4): Conv1d(300, 256, kernel_size=(5,), stride=(1,))\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=1024, out_features=14, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Model Function -"
      ],
      "metadata": {
        "id": "kbMcTamK0eNY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAyjpKA7fjkB"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, dataloader):\n",
        "    model.eval()\n",
        "    Y_pred = []\n",
        "    Y_true = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            text = batch['text']\n",
        "            labels = batch['labels']\n",
        "            pred = model(text).cpu()\n",
        "            pred=np.round(pred)\n",
        "            Y_pred.append(pred.numpy())\n",
        "            Y_true.append(labels.cpu().numpy())\n",
        "        Y_pred = np.concatenate(Y_pred, axis = 0)\n",
        "        Y_true = np.concatenate(Y_true, axis = 0)\n",
        "    return Y_pred, Y_true"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make Predictions on the Test Data -"
      ],
      "metadata": {
        "id": "LeCHoA0J0ZNd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnVRcqHQf9Xo"
      },
      "outputs": [],
      "source": [
        "y_pred, y_true = eval_model(trained_model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the Model Performance"
      ],
      "metadata": {
        "id": "J6euSXXo0OMx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omb3wrzfGfa8",
        "outputId": "0a8263fc-5bb2-468f-a98e-4d66f1a629e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy : 0.20710059171597633\n",
            "\n",
            "Classification Report : \n",
            "                                               precision    recall  f1-score   support\n",
            "\n",
            "                                      Obesity       1.00      0.00      0.00        11\n",
            "                                Non.Adherence       0.33      0.14      0.20        14\n",
            "              Developmental.Delay.Retardation       1.00      0.00      0.00         3\n",
            "                       Advanced.Heart.Disease       0.61      0.39      0.47        36\n",
            "                        Advanced.Lung.Disease       0.75      0.18      0.29        17\n",
            "Schizophrenia.and.other.Psychiatric.Disorders       0.88      0.18      0.30        38\n",
            "                                Alcohol.Abuse       0.82      0.67      0.74        21\n",
            "                        Other.Substance.Abuse       0.67      0.36      0.47        11\n",
            "                    Chronic.Pain.Fibromyalgia       0.57      0.46      0.51        28\n",
            "             Chronic.Neurological.Dystrophies       1.00      0.11      0.21        35\n",
            "                              Advanced.Cancer       1.00      0.33      0.50        18\n",
            "                                   Depression       0.54      0.32      0.40        47\n",
            "                                     Dementia       1.00      0.00      0.00        17\n",
            "                                       Unsure       0.00      0.00      0.00        19\n",
            "\n",
            "                                    micro avg       0.65      0.26      0.37       315\n",
            "                                    macro avg       0.73      0.23      0.29       315\n",
            "                                 weighted avg       0.71      0.26      0.34       315\n",
            "                                  samples avg       0.80      0.39      0.35       315\n",
            "\n"
          ]
        }
      ],
      "source": [
        "target_classes =['Obesity', 'Non.Adherence', 'Developmental.Delay.Retardation',\n",
        "               'Advanced.Heart.Disease', 'Advanced.Lung.Disease',\n",
        "               'Schizophrenia.and.other.Psychiatric.Disorders', 'Alcohol.Abuse',\n",
        "               'Other.Substance.Abuse', 'Chronic.Pain.Fibromyalgia',\n",
        "               'Chronic.Neurological.Dystrophies', 'Advanced.Cancer', 'Depression',\n",
        "               'Dementia', 'Unsure']\n",
        "\n",
        "# Collecting the Predcitions and Truth\n",
        "print(\"Test Accuracy : {}\".format(accuracy_score(y_true, y_pred), zero_division=1))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(y_true, y_pred, target_names=target_classes, zero_division=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUC Score"
      ],
      "metadata": {
        "id": "ZVPxX3N91gu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_t = y_true.transpose()\n",
        "y_p = y_pred.transpose()\n",
        "for i in range(len(y_t)):\n",
        "  auc = roc_auc_score(y_t[i], y_p[i], average=None)\n",
        "  print(f'{target_classes[i]} auc score: {auc:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leUguaQA1jnS",
        "outputId": "98ecdd77-56ab-45a9-a5d0-b75cd680eccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obesity auc score: 0.50\n",
            "Non.Adherence auc score: 0.56\n",
            "Developmental.Delay.Retardation auc score: 0.50\n",
            "Advanced.Heart.Disease auc score: 0.66\n",
            "Advanced.Lung.Disease auc score: 0.58\n",
            "Schizophrenia.and.other.Psychiatric.Disorders auc score: 0.59\n",
            "Alcohol.Abuse auc score: 0.82\n",
            "Other.Substance.Abuse auc score: 0.68\n",
            "Chronic.Pain.Fibromyalgia auc score: 0.70\n",
            "Chronic.Neurological.Dystrophies auc score: 0.56\n",
            "Advanced.Cancer auc score: 0.67\n",
            "Depression auc score: 0.61\n",
            "Dementia auc score: 0.50\n",
            "Unsure auc score: 0.49\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}