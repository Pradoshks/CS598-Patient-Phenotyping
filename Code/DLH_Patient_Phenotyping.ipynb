{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qinalan10/cs598-patient-phenotyping/blob/master/Code/DLH_Patient_Phenotyping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a CNN Model for Patient Phenotyping Based on Patient Notes\n",
        "\n",
        "*   Read the file containing the patient notes and annoations\n",
        "*   Clean the text data\n",
        "*   Create Train, Validation and Test Data Split\n",
        "*   Train a Word2Vec Model\n",
        "*   Create Word Embeddings using the Word2Vec Model\n",
        "*   Create CNN Classfier\n",
        "*   Train & Validate the Classifier\n",
        "*   Test Model Perfromance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LMdndDXF8WaH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcVwbjZ1SlkC",
        "outputId": "3d6929e5-6462-4b99-d924-000a609fac29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# importing libaries\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import pandas as pd \n",
        "from torchsummary import summary\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import re \n",
        "import numpy as np \n",
        "from sklearn.metrics import classification_report\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score   \n",
        "from torchsummary import summary\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use GPU if Available"
      ],
      "metadata": {
        "id": "OsYlMENK_W0A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDVy_8aZt5fA",
        "outputId": "9cc597c5-9ca3-47a6-8564-1d9186508b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting Paths & Read the Final File"
      ],
      "metadata": {
        "id": "M6pll-np_auZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5LFSFksFq9uh"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Data/NOTEEVENTS.csv'\n",
        "annotations_path = \"/content/drive/MyDrive/Data/annotations.csv\"\n",
        "final_file_path = \"/content/drive/MyDrive/Data/final_file.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cepCi-AEai1d"
      },
      "outputs": [],
      "source": [
        "dataframe = pd.read_csv(final_file_path)\n",
        "dataframe.drop('Unnamed: 0', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JmFUPvB7sI8F"
      },
      "outputs": [],
      "source": [
        "class HealthcareTextDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.data = dataframe\n",
        "        self.text_data = self.data['TEXT']\n",
        "        self.labels = torch.tensor(np.array(self.data.iloc[:, :-1]), dtype = torch.float, device=device)  # assuming the labels are in the first 14 columns\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_data.iloc[idx]\n",
        "        labels = self.labels[idx]\n",
        "        sample = {'text': text, 'labels': labels}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6mSpEsjxhcf",
        "outputId": "c1eea7e9-432d-47e3-d7e4-41424472c83f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'uncle', 'grandmother', 'grandfather']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Stop words\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stp_wrds = stopwords.words('english')\n",
        "stp_wrds.extend([\"uncle\", \"grandmother\", \"grandfather\"])\n",
        "print(stp_wrds)\n",
        "stop_words = set(stp_wrds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4Hp1jEhusLY4"
      },
      "outputs": [],
      "source": [
        "# Tokenize the data\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "import string\n",
        "# Define a custom data transformation function\n",
        "def transform_fn(sample):\n",
        "    text = sample['text']\n",
        "    labels = sample['labels']\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    \n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text) # maybe don't remove, could be important \n",
        "    \n",
        "    # Tokenize text data\n",
        "    text = tokenizer(text)\n",
        "\n",
        "    filtered_words  = []\n",
        "    for word in text:\n",
        "      if word not in stop_words:\n",
        "        filtered_words.append(word)\n",
        "    text = filtered_words\n",
        "\n",
        "    # Convert labels to PyTorch tensor\n",
        "    return {'text': text, 'labels': labels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "2nmY6psTsNYM"
      },
      "outputs": [],
      "source": [
        "dataset = HealthcareTextDataset(dataframe, transform_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FKH587uXsr_N"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    texts = [item['text'] for item in batch]  # Extract the text data from the batch\n",
        "    labels = [item['labels'] for item in batch]  # Extract the labels from the batch\n",
        "\n",
        "    # Find the maximum length of text sequences in the batch\n",
        "    max_len = max([len(text) for text in texts])\n",
        "\n",
        "    # Pad the text sequences with a padding token (e.g., '<pad>') to the maximum length\n",
        "    padded_texts = []\n",
        "    for text in texts:\n",
        "      padded_text = text + ['<pad>'] * (max_len - len(text))\n",
        "      padded_texts.append(padded_text)\n",
        "\n",
        "    # Convert the labels to a PyTorch tensor\n",
        "    labels = torch.stack(labels)\n",
        "\n",
        "    return {'text': padded_texts, 'labels': labels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KZi6aXsLstuT"
      },
      "outputs": [],
      "source": [
        "# Setting up random seed\n",
        "torch.manual_seed(456)\n",
        "\n",
        "# Create train, test and validation split\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a Word2Vec Model"
      ],
      "metadata": {
        "id": "liqPieT8zWQP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "amuFbsrXsveU"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "corpus_processed = []\n",
        "for i in train_dataset:\n",
        "  corpus_processed.append(i['text'])\n",
        "\n",
        "RANDOM_SEED = 23432098\n",
        "VEC_SIZE = 300\n",
        "MIN_COUNT = 5\n",
        "sg = 0 # 0 is CBOW, 1 is skip gram\n",
        "w2v_model = Word2Vec(sentences=corpus_processed, \n",
        "                     vector_size= VEC_SIZE, \n",
        "                     min_count = MIN_COUNT, \n",
        "                     workers=1, \n",
        "                     seed = RANDOM_SEED, \n",
        "                     sg=sg, \n",
        "                     epochs=15,\n",
        "                     window=10,\n",
        "                     negative=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Word Embeddings"
      ],
      "metadata": {
        "id": "Q57yYadV9Rjw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H1LchglsxCb",
        "outputId": "1ae40ee9-072d-45d0-a11b-6850304168d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('morbid', 0.8462918996810913), ('osa', 0.7151551842689514), ('migraines', 0.6937200427055359), ('asthma', 0.6794718503952026), ('apnea', 0.6638763546943665), ('oa', 0.6443200707435608), ('osteoarthritis', 0.6354600191116333), ('hypoventilation', 0.6208170056343079), ('osteoporosis', 0.6116981506347656), ('cataracts', 0.609263002872467)]\n",
            "torch.Size([10315, 300])\n",
            "tensor([[-1.6004e+00, -4.0572e-02,  1.4913e+00,  ...,  5.6209e-01,\n",
            "         -1.4693e+00,  6.5162e-01],\n",
            "        [-8.4426e-01, -3.9578e-01,  2.7766e-01,  ..., -6.4214e-02,\n",
            "          3.8522e-01,  8.1983e-01],\n",
            "        [ 8.6164e-01, -4.8736e-02,  2.9800e-01,  ...,  5.4254e-01,\n",
            "          4.4704e-01,  1.1246e+00],\n",
            "        ...,\n",
            "        [-6.1528e-03, -4.5394e-02, -7.8702e-02,  ...,  6.1781e-02,\n",
            "         -2.5810e-02, -6.7572e-02],\n",
            "        [-1.3252e-01, -3.2614e-02,  1.5242e-01,  ..., -1.7464e-01,\n",
            "          9.4650e-02, -5.6039e-03],\n",
            "        [ 1.0066e-01,  4.4941e-04, -1.7577e-01,  ..., -1.9175e-01,\n",
            "          1.4803e-01, -6.0401e-02]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "kv = w2v_model.wv\n",
        "kv.vectors\n",
        "#then convert to kv.vectors to tensor\n",
        "embeddings = torch.tensor(kv.vectors, device=device)\n",
        "print(embeddings.shape)\n",
        "print(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Function to look at some word similarities to see how well the model has been trained."
      ],
      "metadata": {
        "id": "iKEyR_7c9fdt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "KWN3sDoNwJJZ"
      },
      "outputs": [],
      "source": [
        "#input: model: word2vec model\n",
        "#       word: a single word, e.g. 'heart'\n",
        "#output: list_of_words: a list of words similar to the given word\n",
        "def similar_word(model, word):\n",
        "    \n",
        "    list_of_words = []\n",
        "\n",
        "    # your code here\n",
        "    tup_of_words = model.wv.most_similar(word)\n",
        "    list_of_words = list(map(lambda x: x[0], tup_of_words))\n",
        "    return list_of_words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results look promising."
      ],
      "metadata": {
        "id": "-LcCj91K9hJr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhLietbywKIN",
        "outputId": "51976011-dcd6-4f45-bbe6-378d7dc374cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The words similar to 'cancer' are: 'ca, mets, adenocarcinoma, carcinoma, nsclc, rcc, tumor, xrt, breast, lymphoma'.\n",
            "The words similar to 'obesity' are: 'morbid, osa, migraines, asthma, apnea, oa, osteoarthritis, hypoventilation, osteoporosis, cataracts'.\n",
            "The words similar to 'blood' are: 'dropped, support, pcwpmmhg, sent, high, nailbed, urine, yearmonthday, estimated, surveillance'.\n",
            "The words similar to 'heart' are: 'thrive, multiorgan, resp, cardiac, sedimentation, cardiovascular, respiratory, respitory, repiratory, congestive'.\n"
          ]
        }
      ],
      "source": [
        "word = 'cancer'\n",
        "assert type(similar_word(w2v_model, word)) is list\n",
        "print(\"The words similar to '%s' are: '%s'.\" % (word, ', '.join(similar_word(w2v_model, word))))\n",
        "\n",
        "word = 'obesity'\n",
        "assert type(similar_word(w2v_model, word)) is list\n",
        "print(\"The words similar to '%s' are: '%s'.\" % (word, ', '.join(similar_word(w2v_model, word))))\n",
        "\n",
        "word = 'blood'\n",
        "assert type(similar_word(w2v_model, word)) is list\n",
        "print(\"The words similar to '%s' are: '%s'.\" % (word, ', '.join(similar_word(w2v_model, word))))\n",
        "\n",
        "word = 'heart'\n",
        "assert type(similar_word(w2v_model, word)) is list\n",
        "print(\"The words similar to '%s' are: '%s'.\" % (word, ', '.join(similar_word(w2v_model, word))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J22hypn88nKy",
        "outputId": "284d267e-ea1d-4865-b7fc-68a20e0348dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['po', 'name', 'tablet', 'patient', 'blood', 'sig', 'daily', 'hospital', 'one', 'day', 'discharge', 'history', 'last', 'left', 'pm', 'admission', 'right', 'pain', 'pt', 'date', 'namepattern', 'ct', 'medications', 'times', 'normal', 'chest', 'first', 'given', 'also', 'status', 'hours', 'sp', 'course', 'disease', 'every', 'continued', 'needed', 'home', 'stable', 'well', 'capsule', 'started', 'please', 'care', 'md', 'days', 'two', 'per', 'bid', 'negative', 'qh', 'un', 'hct', 'refills', 'x', 'showed', 'prior', 'stitle', 'time', 'tablets', 'wbc', 'medical', 'likely', 'disp', 'dr', 'seen', 'past', 'acute', 'renal', 'pulmonary', 'glucose', 'iv', 'service', 'patients', 'urine', 'release', 'chronic', 'edema', 'noted', 'followup', 'failure', 'due', 'qd', 'l', 'instructions', 'heart', 'prn', 'exam', 'present', 'artery', 'family', 'plt', 'mild', 'known', 'admitted', 'rbc', 'tid', 'without', 'pressure']\n",
            "10315\n"
          ]
        }
      ],
      "source": [
        "w2v_index = kv.index_to_key\n",
        "print(w2v_index[1:100])\n",
        "print(len(w2v_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "RvuKAvdI8hYM"
      },
      "outputs": [],
      "source": [
        "def word2idx(w2v_index):\n",
        "    word_to_idx = {'<pad>':0}\n",
        "    idx = 1\n",
        "    for word in w2v_index:\n",
        "        if word not in word_to_idx:\n",
        "            word_to_idx[word] = idx\n",
        "            idx += 1\n",
        "    return word_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "MCFqK-sE8jUO"
      },
      "outputs": [],
      "source": [
        "w2v_dictionary = word2idx(w2v_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Qlk0Uggr88OA"
      },
      "outputs": [],
      "source": [
        "idx_0 = torch.zeros(1, 300, device = device)\n",
        "embeddings = torch.cat((idx_0, embeddings))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fitting a CNN Classifier"
      ],
      "metadata": {
        "id": "rhxwjxVh9wSZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "G1Sw8yAdvGls"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random \n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_filters, filter_sizes, embeddings, output_size, w2v_dictionary):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        self.dictionary = w2v_dictionary\n",
        "        self.embedding = nn.Embedding.from_pretrained(embeddings)#(vocab_size, embedding_dim)\n",
        "        self.conv1 = nn.Conv1d(embedding_dim, num_filters, filter_sizes[0])\n",
        "        self.conv2 = nn.Conv1d(embedding_dim, num_filters, filter_sizes[1])\n",
        "        self.conv3 = nn.Conv1d(embedding_dim, num_filters, filter_sizes[2])\n",
        "        self.conv4 = nn.Conv1d(embedding_dim, num_filters, filter_sizes[3])\n",
        "        self.dropout = nn.Dropout(p = 0.1)\n",
        "        self.fc = nn.Linear(len(filter_sizes) * num_filters, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "      # change to indexes, then to tensor\n",
        "        z2 = []\n",
        "        for ls in x:\n",
        "          z1 = []\n",
        "          for word in ls:\n",
        "            if word not in self.dictionary.keys():\n",
        "              random_key = random.sample(list(self.dictionary.keys()), 1)[0]\n",
        "              z1.append(self.dictionary[random_key])\n",
        "            else:\n",
        "              z1.append(self.dictionary[word])\n",
        "          z2.append(z1)  \n",
        "\n",
        "        x = torch.tensor(z2, device = device)\n",
        "        x = self.embedding(x)\n",
        "        x = x.permute(0, 2, 1)  # Permute the dimensions for Conv1d (batch_size, embedding_dim, sequence_length)\n",
        "\n",
        "        x1 = F.relu(self.conv1(x))\n",
        "        x1 = F.max_pool1d(x1 , x1.size(2)).squeeze(2)\n",
        "\n",
        "        x2 = F.relu(self.conv2(x))\n",
        "        x2 = F.max_pool1d(x2 , x2.size(2)).squeeze(2)\n",
        "\n",
        "        x3 = F.relu(self.conv3(x))\n",
        "        x3 = F.max_pool1d(x3 , x3.size(2)).squeeze(2)\n",
        "\n",
        "        x4 = F.relu(self.conv4(x))\n",
        "        x4 = F.max_pool1d(x4 , x4.size(2)).squeeze(2)\n",
        "\n",
        "        out = torch.cat((x1, x2, x3, x4), 1)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        out = torch.sigmoid(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the model Parameters"
      ],
      "metadata": {
        "id": "hDiFLX-W93bG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAw0chqJ7AZS",
        "outputId": "64ff0d64-e52a-453c-aaae-f01b8eb6ce04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "         Embedding-1            [-1, 1600, 300]       3,094,800\n",
            "            Conv1d-2            [-1, 256, 1600]          77,056\n",
            "            Conv1d-3            [-1, 256, 1599]         153,856\n",
            "            Conv1d-4            [-1, 256, 1598]         230,656\n",
            "            Conv1d-5            [-1, 256, 1596]         384,256\n",
            "           Dropout-6                 [-1, 1024]               0\n",
            "            Linear-7                   [-1, 14]          14,350\n",
            "================================================================\n",
            "Total params: 3,954,974\n",
            "Trainable params: 860,174\n",
            "Non-trainable params: 3,094,800\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 16.16\n",
            "Params size (MB): 15.09\n",
            "Estimated Total Size (MB): 31.30\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Parametrs\n",
        "embedding_dim = 300\n",
        "num_filters = 256\n",
        "filter_sizes = [1, 2, 3, 5]\n",
        "output_size = 14\n",
        "\n",
        "model = CNNClassifier(embedding_dim, num_filters, filter_sizes, embeddings, output_size, w2v_dictionary)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Print a model sumary\n",
        "summary(model, input_size=(1600, 10), batch_size=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "k2HirDUt9Jen"
      },
      "outputs": [],
      "source": [
        "# Criterion\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adadelta(model.parameters(), lr=0.05, eps=1e-06)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Model"
      ],
      "metadata": {
        "id": "h_C6XHsq-ZFG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "AIyedpEL9MZi"
      },
      "outputs": [],
      "source": [
        "# Train Model\n",
        "def train_model(model, train_dataloader, val_dataloader, epochs=20, optimizer=optimizer, criterion=criterion):\n",
        "  model.to(device)\n",
        "  for epoch in range(epochs):\n",
        "      model.train()\n",
        "      curr_epoch_loss = []\n",
        "      for batch in tqdm(train_dataloader):\n",
        "          text = batch['text']\n",
        "          labels = batch['labels']#.long().flatten()\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(text)#.flatten()\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          curr_epoch_loss.append(loss.cpu().data.numpy())\n",
        "      print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss):.3f}\")\n",
        "      \n",
        "      # Evaluation\n",
        "      model.eval()\n",
        "      val_loss = 0.0\n",
        "      Y_shuffled, Y_preds, losses = [],[],[]\n",
        "      with torch.no_grad():\n",
        "          for batch in val_dataloader:\n",
        "              text = batch['text']\n",
        "              labels = batch['labels']#.long().flatten()\n",
        "              outputs = model(text)#.flatten()\n",
        "              val_loss += criterion(outputs, labels).item()  \n",
        "              \n",
        "              ##\n",
        "              loss = criterion(outputs, labels)\n",
        "              losses.append(loss.item())\n",
        "\n",
        "              Y_shuffled.append(labels.cpu().numpy())\n",
        "              Y_preds.append(outputs.cpu().numpy())\n",
        "\n",
        "              ###\n",
        "      avg_val_loss = val_loss / len(val_dataloader)\n",
        "\n",
        "      print(f'Epoch [{epoch+1}/{epochs}], Val Loss: {avg_val_loss:.3f}')\n",
        "      #print(\"Valid Acc  : {:.3f}\".format(accuracy_score(Y_shuffled, Y_preds)))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX5XsXpD9Opj",
        "outputId": "83d66195-ce63-4530-f86c-a7cadc0e572b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:08<00:00,  2.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 159: curr_epoch_loss=0.125\n",
            "Epoch [160/170], Val Loss: 0.316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:08<00:00,  2.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 160: curr_epoch_loss=0.124\n",
            "Epoch [161/170], Val Loss: 0.310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:08<00:00,  2.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 161: curr_epoch_loss=0.122\n",
            "Epoch [162/170], Val Loss: 0.313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:08<00:00,  2.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 162: curr_epoch_loss=0.125\n",
            "Epoch [163/170], Val Loss: 0.316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:08<00:00,  2.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 163: curr_epoch_loss=0.120\n",
            "Epoch [164/170], Val Loss: 0.315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:08<00:00,  2.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 164: curr_epoch_loss=0.119\n",
            "Epoch [165/170], Val Loss: 0.311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:08<00:00,  2.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 165: curr_epoch_loss=0.120\n",
            "Epoch [166/170], Val Loss: 0.316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:08<00:00,  2.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 166: curr_epoch_loss=0.120\n",
            "Epoch [167/170], Val Loss: 0.310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:08<00:00,  2.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 167: curr_epoch_loss=0.118\n",
            "Epoch [168/170], Val Loss: 0.311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:08<00:00,  2.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 168: curr_epoch_loss=0.122\n",
            "Epoch [169/170], Val Loss: 0.309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:08<00:00,  2.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 169: curr_epoch_loss=0.118\n",
            "Epoch [170/170], Val Loss: 0.316\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 150\n",
        "trained_model = train_model(model, train_loader, val_loader, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Path\n",
        "PATH = '/content/drive/MyDrive/Data/trained_model.pth'"
      ],
      "metadata": {
        "id": "ZH9LPceZyQws"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Model\n",
        "torch.save(trained_model, PATH)"
      ],
      "metadata": {
        "id": "hKzUxQY7x1sh"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "trained_model = torch.load(PATH)\n",
        "print(trained_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TolPup9E1xFL",
        "outputId": "b52c118d-7627-4da4-b123-6efc5551d8ce"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNClassifier(\n",
            "  (embedding): Embedding(10316, 300)\n",
            "  (conv1): Conv1d(300, 256, kernel_size=(1,), stride=(1,))\n",
            "  (conv2): Conv1d(300, 256, kernel_size=(2,), stride=(1,))\n",
            "  (conv3): Conv1d(300, 256, kernel_size=(3,), stride=(1,))\n",
            "  (conv4): Conv1d(300, 256, kernel_size=(5,), stride=(1,))\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=1024, out_features=14, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Model Function -"
      ],
      "metadata": {
        "id": "kbMcTamK0eNY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "eAyjpKA7fjkB"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, dataloader):\n",
        "    model.eval()\n",
        "    Y_pred = []\n",
        "    Y_true = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            text = batch['text']\n",
        "            labels = batch['labels']\n",
        "            pred = model(text).cpu()\n",
        "            condition = pred>0.5\n",
        "            pred =pred.where(condition, torch.tensor(0.0))\n",
        "            condition = pred==0\n",
        "            pred =pred.where(condition, torch.tensor(1.0))\n",
        "            Y_pred.append(pred)\n",
        "            Y_true.append(labels)\n",
        "    return Y_pred, Y_true"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make Predictions on the Test Data -"
      ],
      "metadata": {
        "id": "LeCHoA0J0ZNd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "MnVRcqHQf9Xo"
      },
      "outputs": [],
      "source": [
        "y_pred, y_true = eval_model(trained_model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the Model Performance"
      ],
      "metadata": {
        "id": "J6euSXXo0OMx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omb3wrzfGfa8",
        "outputId": "ecdaabf8-f4ef-4135-feb3-f171a8da6f9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy : 0.21301775147928995\n",
            "\n",
            "Classification Report : \n",
            "                                               precision    recall  f1-score   support\n",
            "\n",
            "                                      Obesity       1.00      0.00      0.00        11\n",
            "                                Non.Adherence       0.33      0.14      0.20        14\n",
            "              Developmental.Delay.Retardation       1.00      0.00      0.00         3\n",
            "                       Advanced.Heart.Disease       0.62      0.42      0.50        36\n",
            "                        Advanced.Lung.Disease       0.67      0.12      0.20        17\n",
            "Schizophrenia.and.other.Psychiatric.Disorders       0.88      0.18      0.30        38\n",
            "                                Alcohol.Abuse       0.82      0.67      0.74        21\n",
            "                        Other.Substance.Abuse       0.57      0.36      0.44        11\n",
            "                    Chronic.Pain.Fibromyalgia       0.59      0.46      0.52        28\n",
            "             Chronic.Neurological.Dystrophies       0.80      0.11      0.20        35\n",
            "                              Advanced.Cancer       1.00      0.33      0.50        18\n",
            "                                   Depression       0.56      0.32      0.41        47\n",
            "                                     Dementia       1.00      0.00      0.00        17\n",
            "                                       Unsure       0.00      0.00      0.00        19\n",
            "\n",
            "                                    micro avg       0.65      0.26      0.37       315\n",
            "                                    macro avg       0.70      0.22      0.29       315\n",
            "                                 weighted avg       0.68      0.26      0.34       315\n",
            "                                  samples avg       0.80      0.38      0.35       315\n",
            "\n"
          ]
        }
      ],
      "source": [
        "target_classes =['Obesity', 'Non.Adherence', 'Developmental.Delay.Retardation',\n",
        "               'Advanced.Heart.Disease', 'Advanced.Lung.Disease',\n",
        "               'Schizophrenia.and.other.Psychiatric.Disorders', 'Alcohol.Abuse',\n",
        "               'Other.Substance.Abuse', 'Chronic.Pain.Fibromyalgia',\n",
        "               'Chronic.Neurological.Dystrophies', 'Advanced.Cancer', 'Depression',\n",
        "               'Dementia', 'Unsure']\n",
        "\n",
        "# Collecting the Predcitions and Truth\n",
        "Y_pred = torch.vstack((y_pred[0], y_pred[1], y_pred[2]))\n",
        "Y_true = torch.vstack((y_true[0], y_true[1], y_true[2]))\n",
        "print(\"Test Accuracy : {}\".format(accuracy_score(Y_true.cpu(), Y_pred.cpu()), zero_division=1))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_true.cpu().numpy(), Y_pred.cpu(), target_names=target_classes, zero_division=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMOed1bsoMac",
        "outputId": "733e6665-7075-4455-dfaa-d7e84b78f1ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               precision    recall  f1-score   support\n",
            "\n",
            "                                      Obesity       1.00      0.00      0.00         5\n",
            "                                Non.Adherence       0.00      0.00      0.00         4\n",
            "              Developmental.Delay.Retardation       1.00      0.00      0.00         1\n",
            "                       Advanced.Heart.Disease       0.62      0.50      0.55        16\n",
            "                        Advanced.Lung.Disease       0.50      0.11      0.18         9\n",
            "Schizophrenia.and.other.Psychiatric.Disorders       0.83      0.33      0.48        15\n",
            "                                Alcohol.Abuse       0.80      0.67      0.73         6\n",
            "                        Other.Substance.Abuse       0.67      0.50      0.57         4\n",
            "                    Chronic.Pain.Fibromyalgia       0.67      0.50      0.57        12\n",
            "             Chronic.Neurological.Dystrophies       1.00      0.00      0.00        12\n",
            "                              Advanced.Cancer       1.00      0.20      0.33         5\n",
            "                                   Depression       0.73      0.42      0.53        19\n",
            "                                     Dementia       1.00      0.00      0.00         4\n",
            "                                       Unsure       0.00      0.00      0.00         8\n",
            "\n",
            "                                    micro avg       0.65      0.29      0.40       120\n",
            "                                    macro avg       0.70      0.23      0.28       120\n",
            "                                 weighted avg       0.69      0.29      0.36       120\n",
            "                                  samples avg       0.80      0.41      0.34       120\n",
            "\n",
            "                                               precision    recall  f1-score   support\n",
            "\n",
            "                                      Obesity       1.00      0.00      0.00         4\n",
            "                                Non.Adherence       1.00      0.20      0.33         5\n",
            "              Developmental.Delay.Retardation       1.00      0.00      0.00         1\n",
            "                       Advanced.Heart.Disease       0.86      0.50      0.63        12\n",
            "                        Advanced.Lung.Disease       1.00      0.33      0.50         3\n",
            "Schizophrenia.and.other.Psychiatric.Disorders       1.00      0.11      0.20        18\n",
            "                                Alcohol.Abuse       1.00      0.70      0.82        10\n",
            "                        Other.Substance.Abuse       0.50      0.25      0.33         4\n",
            "                    Chronic.Pain.Fibromyalgia       0.50      0.40      0.44        10\n",
            "             Chronic.Neurological.Dystrophies       0.67      0.13      0.22        15\n",
            "                              Advanced.Cancer       1.00      0.33      0.50         9\n",
            "                                   Depression       0.55      0.35      0.43        17\n",
            "                                     Dementia       1.00      0.00      0.00         8\n",
            "                                       Unsure       1.00      0.00      0.00         5\n",
            "\n",
            "                                    micro avg       0.73      0.27      0.40       121\n",
            "                                    macro avg       0.86      0.24      0.32       121\n",
            "                                 weighted avg       0.82      0.27      0.36       121\n",
            "                                  samples avg       0.87      0.39      0.39       121\n",
            "\n",
            "                                               precision    recall  f1-score   support\n",
            "\n",
            "                                      Obesity       1.00      0.00      0.00         2\n",
            "                                Non.Adherence       0.50      0.20      0.29         5\n",
            "              Developmental.Delay.Retardation       1.00      0.00      0.00         1\n",
            "                       Advanced.Heart.Disease       0.25      0.12      0.17         8\n",
            "                        Advanced.Lung.Disease       1.00      0.00      0.00         5\n",
            "Schizophrenia.and.other.Psychiatric.Disorders       1.00      0.00      0.00         5\n",
            "                                Alcohol.Abuse       0.60      0.60      0.60         5\n",
            "                        Other.Substance.Abuse       0.50      0.33      0.40         3\n",
            "                    Chronic.Pain.Fibromyalgia       0.60      0.50      0.55         6\n",
            "             Chronic.Neurological.Dystrophies       1.00      0.25      0.40         8\n",
            "                              Advanced.Cancer       1.00      0.50      0.67         4\n",
            "                                   Depression       0.20      0.09      0.13        11\n",
            "                                     Dementia       1.00      0.00      0.00         5\n",
            "                                       Unsure       0.00      0.00      0.00         6\n",
            "\n",
            "                                    micro avg       0.50      0.19      0.27        74\n",
            "                                    macro avg       0.69      0.19      0.23        74\n",
            "                                 weighted avg       0.61      0.19      0.24        74\n",
            "                                  samples avg       0.70      0.34      0.30        74\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_true[0].cpu().numpy(), y_pred[0].cpu(), target_names=target_classes, zero_division=1))\n",
        "print(classification_report(y_true[1].cpu().numpy(), y_pred[1].cpu(), target_names=target_classes, zero_division=1))\n",
        "print(classification_report(y_true[2].cpu().numpy(), y_pred[2].cpu(), target_names=target_classes, zero_division=1))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}